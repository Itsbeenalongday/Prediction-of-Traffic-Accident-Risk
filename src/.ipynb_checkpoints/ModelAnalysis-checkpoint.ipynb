{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1\n",
    "# Prediction of Traffic Accident Risk\n",
    "Team 7: 어서오십쇼HUMAN  \n",
    "Editor: 유성민, 김도운"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2\n",
    "## 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        TargetOrigin  Target_0  Target_1  Target_2  Target_3  Location_East  \\\n",
      "201566             2         0         0         1         0              0   \n",
      "357360             2         0         0         1         0              0   \n",
      "225613             2         0         0         1         0              0   \n",
      "17123              2         0         0         1         0              0   \n",
      "223917             2         0         0         1         0              0   \n",
      "...              ...       ...       ...       ...       ...            ...   \n",
      "79587              1         0         1         0         0              0   \n",
      "213575             0         1         0         0         0              0   \n",
      "217201             0         1         0         0         0              0   \n",
      "216797             0         1         0         0         0              1   \n",
      "212965             0         1         0         0         0              1   \n",
      "\n",
      "        Location_North  Location_South  Location_West  RoadState_건조  ...  \\\n",
      "201566               1               0              0             1  ...   \n",
      "357360               0               1              0             1  ...   \n",
      "225613               1               0              0             0  ...   \n",
      "17123                1               0              0             1  ...   \n",
      "223917               0               0              1             1  ...   \n",
      "...                ...             ...            ...           ...  ...   \n",
      "79587                1               0              0             1  ...   \n",
      "213575               0               0              1             1  ...   \n",
      "217201               1               0              0             1  ...   \n",
      "216797               0               0              0             1  ...   \n",
      "212965               0               0              0             1  ...   \n",
      "\n",
      "        Time_afternoon  Time_night  AgeBand_10  AgeBand_20  AgeBand_30  \\\n",
      "201566               0           1           0           0           0   \n",
      "357360               1           0           0           0           0   \n",
      "225613               0           0           0           1           0   \n",
      "17123                1           0           0           0           0   \n",
      "223917               0           0           0           1           0   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "79587                0           1           0           1           0   \n",
      "213575               0           0           0           1           0   \n",
      "217201               1           0           0           0           0   \n",
      "216797               1           0           0           0           0   \n",
      "212965               1           0           0           0           0   \n",
      "\n",
      "        AgeBand_40  AgeBand_50  AgeBand_60  AgeBand_70  AgeBand_80  \n",
      "201566           0           1           0           0           0  \n",
      "357360           0           0           1           0           0  \n",
      "225613           0           0           0           0           0  \n",
      "17123            0           1           0           0           0  \n",
      "223917           0           0           0           0           0  \n",
      "...            ...         ...         ...         ...         ...  \n",
      "79587            0           0           0           0           0  \n",
      "213575           0           0           0           0           0  \n",
      "217201           0           0           0           1           0  \n",
      "216797           1           0           0           0           0  \n",
      "212965           0           0           0           0           1  \n",
      "\n",
      "[12000 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "preprocessing = pd.read_csv('../input/preprocessing.csv')\n",
    "\n",
    "preprocessing = preprocessing.iloc[np.random.permutation(len(preprocessing))]\n",
    "\n",
    "y = preprocessing.loc[:, preprocessing.columns == 'Target']\n",
    "x = preprocessing.loc[:, preprocessing.columns != 'Target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "print(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "count0, count1, count2, count3 = 0, 0, 0, 0\n",
    "for i in y_test:\n",
    "    if i == 0: count0 += 1\n",
    "    elif i == 1: count1 += 1\n",
    "    elif i == 2: count2 += 1\n",
    "    else: count3 += 1\n",
    "print(count0, count1, count2, count3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5\n",
    "# Do you need standardization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "//6\n",
       "var input = prompt('Do you need standardization? then if you want input y', 'waiting your input ......');\n",
       "var i;\n",
       "if(input == 'y'){\n",
       "     alert('표준화를 진행합니다');\n",
       "    Jupyter.notebook.execute_cells([3]); // read data\n",
       "     for(i=35; i<=61; i++) {\n",
       "        Jupyter.notebook.execute_cells([i]); // learning\n",
       "     }\n",
       "}else{\n",
       "    alert('표준화 없이 진행합니다.');\n",
       "      Jupyter.notebook.execute_cells([3]); // read data\n",
       "     for(i=7; i<=34; i++) {\n",
       "        Jupyter.notebook.execute_cells([i]); // learning\n",
       "     }\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "//6\n",
    "var input = prompt('Do you need standardization? then if you want input y', 'waiting your input ......');\n",
    "var i;\n",
    "if(input == 'y'){\n",
    "     alert('표준화를 진행합니다');\n",
    "    Jupyter.notebook.execute_cells([3]); // read data\n",
    "     for(i=35; i<=61; i++) {\n",
    "        Jupyter.notebook.execute_cells([i]); // learning\n",
    "     }\n",
    "}else{\n",
    "    alert('표준화 없이 진행합니다.');\n",
    "      Jupyter.notebook.execute_cells([3]); // read data\n",
    "     for(i=7; i<=34; i++) {\n",
    "        Jupyter.notebook.execute_cells([i]); // learning\n",
    "     }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7\n",
    "## 2. 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#8\n",
    "# Prepare Model training and analysis\n",
    "\n",
    "> editor: seongminyoo   \n",
    "> Date of modification: 2020-05-25 04:49pm\n",
    "\n",
    "`solving problem`\n",
    "+ Classification Problem\n",
    "\n",
    "`using model`\n",
    "+ KNN\n",
    "+ Decision Tree\n",
    "+ Bagging\n",
    "+ Random Forest\n",
    "+ SVM\n",
    "+ Neural Network classifier\n",
    "\n",
    "`algorithm for each step`\n",
    "1. GridSearchCV by 10 fold or 5 fold\n",
    "    - make parameter set\n",
    "2. Test\n",
    "    - best_esitimater is model that construct best hyper parameter set\n",
    "3. Analysis result\n",
    "    - use confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#9\n",
    "# Make functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "# 최고 모델 만드는 함수\n",
    "def make_model(model, param_grid, cv):\n",
    "    try: # random_state 가능 모델\n",
    "        gs = GridSearchCV(estimator=model(random_state = 1), param_grid=param_grid, scoring='accuracy', cv=cv)\n",
    "    except Exception: # random_state 불가능 모델\n",
    "        print('error detection!')\n",
    "        gs = GridSearchCV(estimator=model(), param_grid=param_grid, scoring='accuracy', cv=cv)\n",
    "    finally:\n",
    "        gs.fit(x_train,y_train)\n",
    "        best_params = gs.best_params_\n",
    "        print(best_params)\n",
    "        return gs\n",
    "\n",
    "# test function\n",
    "def make_test(model):\n",
    "        best_model=model.best_estimator_\n",
    "        y_pred = best_model.predict(x_test)\n",
    "        print(classification_report(y_test,y_pred,target_names=['class 0','class 1','class 2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11\n",
    "from concurrent.futures import as_completed, ProcessPoolExecutor\n",
    "import time\n",
    "import numpy as np\n",
    "import winprocess\n",
    "\n",
    "def parallel_processing(model, param_grid, cv):\n",
    "    executor = ProcessPoolExecutor(max_workers=50)\n",
    "    fs = winprocess.submit(executor, make_model,model, param_grid, cv)\n",
    "    return fs.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#12\n",
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13\n",
    "# hyper parameter sets\n",
    "param_range = [i for i in range(1,5)]\n",
    "param_grid = [\n",
    "    {'n_neighbors': param_range, 'metric': ['euclidean']},\n",
    "    {'n_neighbors': param_range, 'metric': ['manhattan']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14\n",
    "best_model = parallel_processing(KNeighborsClassifier,param_grid, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#16\n",
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17\n",
    "# hyper parameter sets\n",
    "param_range = [i for i in range(3,7)]\n",
    "# 1 아래의 parmeter들과 위의 k가 한번 씩 모두 들어가서 brute-force를 진행함으로써 제일 좋은 파라미터를  결과로 도출해준다.\n",
    "param_grid = [\n",
    "    {'max_depth': param_range, 'criterion': ['entropy']},\n",
    "    {'max_depth': param_range, 'criterion': ['gini']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18\n",
    "# Decision Tree train and test\n",
    "best_model = parallel_processing(tree.DecisionTreeClassifier,param_grid,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.46      0.48      0.47       128\n",
      "     class 2       0.68      0.41      0.51      1783\n",
      "     class 3       0.92      0.97      0.94     12539\n",
      "\n",
      "    accuracy                           0.90     14450\n",
      "   macro avg       0.69      0.62      0.64     14450\n",
      "weighted avg       0.88      0.90      0.89     14450\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [14450, 1800]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4af0a0ecf39b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Confusion matrix: \\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\유성민\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\유성민\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \"\"\"\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\유성민\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \"\"\"\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\유성민\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    257\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [14450, 1800]"
     ]
    }
   ],
   "source": [
    "#19\n",
    "make_test(best_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#20\n",
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21\n",
    "# hyper parameter set for Random Forest\n",
    "estimators = [10, 100]\n",
    "depth_range = [i for i in range(3,7)]\n",
    "param_grid = [\n",
    "    {'max_depth': depth_range, 'criterion': ['entropy'], 'n_estimators':estimators},\n",
    "    {'max_depth': depth_range, 'criterion': ['gini'], 'n_estimators':estimators}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#22\n",
    "# Random Forest train and test\n",
    "best_model = parallel_processing(RandomForestClassifier,param_grid,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.00      0.00      0.00       145\n",
      "     class 2       0.00      0.00      0.00      1786\n",
      "     class 3       0.87      1.00      0.93     12519\n",
      "\n",
      "    accuracy                           0.87     14450\n",
      "   macro avg       0.29      0.33      0.31     14450\n",
      "weighted avg       0.75      0.87      0.80     14450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#23\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#24\n",
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#25\n",
    "# hyper parameter set for Bagging\n",
    "estimators = [10, 100]\n",
    "depth_range = [i for i in range(3,7)]\n",
    "param_grid = [\n",
    "    {'base_estimator__max_depth': depth_range, 'base_estimator__criterion': ['entropy'], 'n_estimators':estimators},\n",
    "    {'base_estimator__max_depth': depth_range, 'base_estimator__criterion': ['gini'], 'n_estimators':estimators}\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_estimator__criterion': 'gini', 'base_estimator__max_depth': 6, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.00      0.00      0.00       145\n",
      "     class 2       1.00      0.00      0.00      1786\n",
      "     class 3       0.87      1.00      0.93     12519\n",
      "\n",
      "    accuracy                           0.87     14450\n",
      "   macro avg       0.62      0.33      0.31     14450\n",
      "weighted avg       0.87      0.87      0.80     14450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#26\n",
    "#Bagging train and test\n",
    "\n",
    "gs = GridSearchCV(estimator=BaggingClassifier(tree.DecisionTreeClassifier(random_state = 1),random_state=1)\n",
    "                  , param_grid=param_grid, scoring='accuracy', cv=5,)\n",
    "gs.fit(x_train,y_train)\n",
    "print(gs.best_params_)\n",
    "best_bg=gs.best_estimator_\n",
    "y_pred = best_bg.predict(x_test) \n",
    "print(classification_report(y_test,y_pred,target_names=['class 1','class 2','class 3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#27\n",
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#28\n",
    "# hyper parameter set for SVM\n",
    "param_range = [0.01, 1 ,10.0]\n",
    "param_grid = [\n",
    "    {'C': param_range, 'kernel': ['linear']},\n",
    "    {'C': param_range, 'kernel': ['rbf']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#29\n",
    "# SVM Forest train and test\n",
    "best_model = parallel_processing(SVC,param_grid,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#31\n",
    "# Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#32\n",
    "# hyper parameter set for Neural Network\n",
    "param_learning_rate=['constant', 'adaptive']\n",
    "param_activation = ['relu', 'identity', 'tanh', 'logistic']\n",
    "param_solver=['sgd', 'adam']\n",
    "param_hidden_layer_sizes=[(100,)]\n",
    "param_grid = [\n",
    "    {'learning_rate': param_learning_rate, 'hidden_layer_sizes':param_hidden_layer_sizes, \n",
    "     'activation':param_activation, 'solver':param_solver}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#33\n",
    "# Neural Network train and test\n",
    "best_model = parallel_processing(MLPClassifier,param_grid, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#34\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#35\n",
    "# Prepare Model training and analysis\n",
    "\n",
    "> editor: seongminyoo   \n",
    "> Date of modification: 2020-05-25 04:49pm\n",
    "\n",
    "`solving problem`\n",
    "+ Classification Problem\n",
    "\n",
    "`using model`\n",
    "+ KNN\n",
    "+ Decision Tree\n",
    "+ Bagging\n",
    "+ Random Forest\n",
    "+ SVM\n",
    "+ Neural Network classifier\n",
    "\n",
    "`algorithm for each step`\n",
    "1. Make pipeline\n",
    "    - standard data, model\n",
    "2. GridSearchCV by 10 fold or 5 fold\n",
    "    - make parameter set\n",
    "3. Test\n",
    "    - best_esitimater is model that construct best hyper parameter set\n",
    "4. Analysis result\n",
    "    - use confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#36\n",
    "# Make functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#37\n",
    "def make_model(model_name, model, param_grid, cv):\n",
    "    try: # random_state 가능 모델\n",
    "        pipe_svc = Pipeline([('scl', StandardScaler()), (model_name,model(random_state = 1))])\n",
    "    except Exception: # random_state 불가능 모델\n",
    "        print('error detection!')\n",
    "        pipe_svc = Pipeline([('scl', StandardScaler()), (model_name,model())])\n",
    "    finally:\n",
    "        gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=cv)\n",
    "        gs.fit(x_train,y_train)\n",
    "        best_params = gs.best_params_\n",
    "        print(best_params)\n",
    "        return gs\n",
    "\n",
    "# test function\n",
    "def make_test(model):\n",
    "        best_model=model.best_estimator_\n",
    "        y_pred = best_model.predict(x_test)\n",
    "        print(classification_report(y_test,y_pred,target_names=['class 1','class 2','class 3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#38\n",
    "from concurrent.futures import as_completed, ProcessPoolExecutor\n",
    "import time\n",
    "import numpy as np\n",
    "import winprocess\n",
    "\n",
    "def parallel_processing(model_name, model, param_grid, cv):\n",
    "    executor = ProcessPoolExecutor(max_workers=50)\n",
    "    fs = winprocess.submit(executor, make_model, model_name, model, param_grid, cv)\n",
    "    return fs.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#39\n",
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#40\n",
    "# hyper parameter sets\n",
    "param_range = [i for i in range(1,11)]\n",
    "param_grid = [\n",
    "    {'knn__n_neighbors': param_range, 'knn__metric': ['euclidean']},\n",
    "    {'knn__n_neighbors': param_range, 'knn__metric': ['manhattan']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#41\n",
    "best_model = parallel_processing('knn',KNeighborsClassifier,param_grid, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#42\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#43\n",
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#44\n",
    "# hyper parameter sets\n",
    "param_range = [i for i in range(3,7)]\n",
    "# 1 아래의 parmeter들과 위의 k가 한번 씩 모두 들어가서 brute-force를 진행함으로써 제일 좋은 파라미터를  결과로 도출해준다.\n",
    "param_grid = [\n",
    "    {'dt__max_depth': param_range, 'dt__criterion': ['entropy']},\n",
    "    {'dt__max_depth': param_range, 'dt__criterion': ['gini']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#45\n",
    "# Decision Tree train and test\n",
    "best_model = parallel_processing('dt',tree.DecisionTreeClassifier,param_grid,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#46\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#47\n",
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#48\n",
    "# hyper parameter set for Random Forest\n",
    "estimators = [10,100]\n",
    "depth_range = [i for i in range(3,7)]\n",
    "param_grid = [\n",
    "    {'rf__max_depth': depth_range, 'rf__criterion': ['entropy'], 'rf__n_estimators':estimators},\n",
    "    {'rf__max_depth': depth_range, 'rf__criterion': ['gini'], 'rf__n_estimators':estimators}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#49\n",
    "# Random Forest train and test\n",
    "best_model = parallel_processing('rf',RandomForestClassifier,param_grid,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#51\n",
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#52\n",
    "# hyper parameter set for Bagging\n",
    "estimators = [10, 100]\n",
    "depth_range = [i for i in range(3,7)]\n",
    "param_grid = [\n",
    "    {'bg__base_estimator__max_depth': depth_range, 'bg__base_estimator__criterion': ['entropy'], 'bg__n_estimators':estimators},\n",
    "    {'bg__base_estimator__max_depth': depth_range, 'bg__base_estimator__criterion': ['gini'], 'bg__n_estimators':estimators}\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#53\n",
    "#Bagging train and test\n",
    "pipe_svc = Pipeline([('scl', StandardScaler()), ('bg',BaggingClassifier(tree.DecisionTreeClassifier(random_state = 1),random_state=1))])\n",
    "gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=5,)\n",
    "gs.fit(x_train,y_train)\n",
    "print(gs.best_params_)\n",
    "best_bg=gs.best_estimator_\n",
    "y_pred = best_bg.predict(x_test) \n",
    "print(classification_report(y_test,y_pred,target_names=['lsclass 1','class 2','class 3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#54\n",
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#55\n",
    "# hyper parameter set for SVM\n",
    "param_range = [0.01, 1 ,10.0]\n",
    "param_grid = [\n",
    "    {'clf__C': param_range, 'clf__kernel': ['linear']},\n",
    "    {'clf__C': param_range, 'clf__kernel': ['rbf']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#56\n",
    "# SVM Forest train and test\n",
    "best_model = parallel_processing('clf',SVC,param_grid,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#57\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#58\n",
    "# Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#59\n",
    "# hyper parameter set for Neural Network\n",
    "param_learning_rate=['constant', 'adaptive']\n",
    "param_activation = ['relu', 'identity', 'tanh', 'logistic']\n",
    "param_solver=['sgd', 'adam']\n",
    "param_hidden_layer_sizes=[(100,)]\n",
    "param_grid = [\n",
    "    {'nnc__learning_rate': param_learning_rate, 'nnc__hidden_layer_sizes':param_hidden_layer_sizes, \n",
    "     'nnc__activation':param_activation, 'nnc__solver':param_solver}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#60\n",
    "# Neural Network train and test\n",
    "best_model = parallel_processing('nnc',MLPClassifier,param_grid, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#61\n",
    "make_test(best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
