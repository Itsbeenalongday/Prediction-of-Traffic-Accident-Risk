{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Traffic Accident Risk\n",
    "Team 7: 어서오십쇼HUMAN  \n",
    "Editor: 유성민, 김도운\n",
    "## 2. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 준비\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import warnings\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set(font_scale=2.5)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('../input/sample_accident.csv')\n",
    "\n",
    "print(\"sample.shape:\", sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = pd.read_csv('../input/preprocessing.csv')\n",
    "\n",
    "print(\"sample.shape:\", preprocessing.shape)\n",
    "\n",
    "# 초기 샘플 데이터\n",
    "preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date, age 제외하기, target = 0,1,2,3으로 변경\n",
    "data1 = preprocessing.loc[ : , \"Time\" : \"Type\"]\n",
    "data2 = preprocessing.loc[:, \"AgeBand\"]\n",
    "data = pd.concat([data1,data2], axis = 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 타겟 데이터를 트레인 테스트 분리하기 전에 X, y를 배열 형태로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model training and analysis\n",
    "\n",
    "> editor: seongminyoo   \n",
    "> Date of modification: 2020-05-25 04:49pm\n",
    "\n",
    "`solving problem`\n",
    "+ Classification Problem\n",
    "\n",
    "`using model`\n",
    "+ KNN\n",
    "+ Decision Tree\n",
    "+ Bagging\n",
    "+ Random Forest\n",
    "+ SVM\n",
    "+ Neural Network classifier\n",
    "\n",
    "`algorithm for each step`\n",
    "1. Make pipeline\n",
    "    - standard data, model\n",
    "2. GridSearchCV by 10 fold or 5 fold\n",
    "    - make parameter set\n",
    "3. Test\n",
    "    - best_esitimater is model that construct best hyper parameter set\n",
    "4. Analysis result\n",
    "    - use confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split\n",
    "# target 0,1,2,3으로 수정필요\n",
    "# target data를 array 형태로 나타내야됨(y 수정 필요)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, y_train, x_test, y_test = train_test_split(X, y, test_size = 0.1, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import using package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최고 모델 만드는 함수\n",
    "def make_model(model_name, model, param_grid, cv):\n",
    "    try: # random_state 가능 모델\n",
    "        pipe_svc = Pipeline([('scl', StandardScaler()), (model_name,model(random_state = 1))])\n",
    "    except Exception: # random_state 불가능 모델\n",
    "        print('error detection!')\n",
    "        pipe_svc = Pipeline([('scl', StandardScaler()), (model_name,model())])\n",
    "    finally:\n",
    "        gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "        gs.fit(x_train,y_train)\n",
    "        best_params = gs.best_params_\n",
    "        print(best_params)\n",
    "        return gs\n",
    "\n",
    "# test function\n",
    "def make_test(model):\n",
    "        best_model=model.best_estimator_\n",
    "        y_pred = best_model.predict(x_test)\n",
    "        print(classification_report(y_test,y_pred,target_names=['class 0','class 1','class 2', 'class 3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy data for Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dummy data for debug\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X, y = make_classification(n_features=8, n_informative=5,\n",
    "#                            n_redundant=3, n_clusters_per_class=1, random_state=4, n_samples =1000 )\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter sets\n",
    "param_range = [1000,2000,3000,4000,5000] # dummy데이터보다 이웃수가 더 많아서 테스트 못함\n",
    "# param_range = [i for i in range(1,11)]\n",
    "param_grid = [\n",
    "    {'knn__n_neighbors': param_range, 'knn__metric': ['euclidean']},\n",
    "    {'knn__n_neighbors': param_range, 'knn__metric': ['manhattan']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error detection!\n"
     ]
    }
   ],
   "source": [
    "# KNN train and test\n",
    "make_test(make_model('knn',KNeighborsClassifier,param_grid,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter sets\n",
    "param_range = [i for i in range(5,21)]\n",
    "# 1 아래의 parmeter들과 위의 k가 한번 씩 모두 들어가서 brute-force를 진행함으로써 제일 좋은 파라미터를  결과로 도출해준다.\n",
    "param_grid = [\n",
    "    {'dt__max_depth': param_range, 'dt__criterion': ['entropy']},\n",
    "    {'dt__max_depth': param_range, 'dt__criterion': ['gini']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dt__criterion': 'gini', 'dt__max_depth': 6}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.96      0.98      0.97        54\n",
      "     class 1       0.98      0.96      0.97        46\n",
      "\n",
      "    accuracy                           0.97       100\n",
      "   macro avg       0.97      0.97      0.97       100\n",
      "weighted avg       0.97      0.97      0.97       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree train and test\n",
    "make_test(make_model('dt',tree.DecisionTreeClassifier,param_grid,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter set for Random Forest\n",
    "estimators = [10, 100 , 500]\n",
    "depth_range = [i for i in range(5,11)]\n",
    "param_grid = [\n",
    "    {'rf__max_depth': depth_range, 'rf__criterion': ['entropy'], 'rf__n_estimators':estimators},\n",
    "    {'rf__max_depth': depth_range, 'rf__criterion': ['gini'], 'rf__n_estimators':estimators}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rf__criterion': 'entropy', 'rf__max_depth': 9, 'rf__n_estimators': 500}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.98      0.96      0.97        53\n",
      "     class 1       0.96      0.98      0.97        47\n",
      "\n",
      "    accuracy                           0.97       100\n",
      "   macro avg       0.97      0.97      0.97       100\n",
      "weighted avg       0.97      0.97      0.97       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest train and test\n",
    "make_test(make_model('rf',RandomForestClassifier,param_grid,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter set for Bagging\n",
    "estimators = [10,100, 500]\n",
    "depth_range = [i for i in range(5,11)]\n",
    "param_grid = [\n",
    "    {'bg__base_estimator__max_depth': depth_range, 'bg__base_estimator__criterion': ['entropy'], 'bg__n_estimators':estimators},\n",
    "    {'bg__base_estimator__max_depth': depth_range, 'bg__base_estimator__criterion': ['gini'], 'bg__n_estimators':estimators}\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bg__base_estimator__criterion': 'entropy', 'bg__base_estimator__max_depth': 6, 'bg__n_estimators': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.98      1.00      0.99        54\n",
      "     class 1       1.00      0.98      0.99        46\n",
      "\n",
      "    accuracy                           0.99       100\n",
      "   macro avg       0.99      0.99      0.99       100\n",
      "weighted avg       0.99      0.99      0.99       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Bagging train and test\n",
    "pipe_svc = Pipeline([('scl', StandardScaler()), ('bg',BaggingClassifier(tree.DecisionTreeClassifier(random_state = 1),random_state=1))])\n",
    "gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "gs.fit(x_train,y_train)\n",
    "print(gs.best_params_)\n",
    "best_bg=gs.best_estimator_\n",
    "y_pred = best_bg.predict(x_test) \n",
    "print(classification_report(y_test,y_pred,target_names=['class 0','class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter set for SVM\n",
    "param_range = [10e-6, 10e-4, 0.01, 1 ,10.0]\n",
    "param_grid = [\n",
    "    {'clf__C': param_range, 'clf__kernel': ['linear']},\n",
    "    {'clf__C': param_range, 'clf__kernel': ['rbf']},\n",
    "    {'clf__C': param_range, 'clf__kernel': ['poly']},\n",
    "    {'clf__C': param_range, 'clf__kernel': ['sigmid']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 10.0, 'clf__kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.98      1.00      0.99        54\n",
      "     class 1       1.00      0.98      0.99        46\n",
      "\n",
      "    accuracy                           0.99       100\n",
      "   macro avg       0.99      0.99      0.99       100\n",
      "weighted avg       0.99      0.99      0.99       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_test(make_model('clf',SVC,param_grid,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter set for Neural Network\n",
    "param_grid = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network test\n",
    "make_test(make_model('nnc',MLPClassifier,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
